{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# make sure the project modules can be found\n",
    "src_path = Path(os.getcwd()).parent.parent.absolute()\n",
    "sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import jiwer\n",
    "import gc\n",
    "\n",
    "def wer(predictions: list[str], references: list[str], chunk_size: Optional[int] = None) -> float:\n",
    "    \"\"\"\n",
    "    Computes WER score of transcribed segments against references.\n",
    "\n",
    "    Word error rate (WER) is a common metric of the performance of an automatic speech recognition system.\n",
    "    The general difficulty of measuring performance lies in the fact that the recognized word sequence can \n",
    "    have a different length from the reference word sequence (supposedly the correct one). \n",
    "    The WER is derived from the Levenshtein distance, working at the word level instead of the phoneme level.\n",
    "\n",
    "    Word error rate can then be computed as:\n",
    "    WER = (S + D + I) / N = (S + D + I) / (S + D + C)\n",
    "    where\n",
    "    S is the number of substitutions,\n",
    "    D is the number of deletions,\n",
    "    I is the number of insertions,\n",
    "    C is the number of correct words,\n",
    "    N is the number of words in the reference (N=S+D+C).\n",
    "\n",
    "    WER's output is always a number between 0 and 1. This value indicates the percentage of words that were incorrectly predicted. \n",
    "    The lower the value, the better the performance of the ASR system with a WER of 0 being a perfect score.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        predictions: list[str]\n",
    "            List of transcribtions to score.\n",
    "        \n",
    "        references: list[str]\n",
    "            List of references for each speech input.\n",
    "        \n",
    "        chunk_size: Optional[int] = None\n",
    "            Size of the chunk to use for computation. \n",
    "            When this value is specified, the function will chunk the data into batches of the specified size and compute the WER on each batch.\n",
    "            After all batches are computed, the function will compute the average WER over all batches.\n",
    "            (You will probably need to define this if you have memory issues).\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "        float: the word error rate\n",
    "    \"\"\"\n",
    "    \n",
    "    if chunk_size is None: return jiwer.wer(references, predictions)\n",
    "    \n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "    H, S, D, I = 0, 0, 0, 0\n",
    "\n",
    "    while start < len(references):\n",
    "\n",
    "        chunk_metrics = jiwer.compute_measures(references[start:end], predictions[start:end], \n",
    "                                               truth_transform=jiwer.transformations.wer_default, \n",
    "                                               hypothesis_transform=jiwer.transformations.wer_default)\n",
    "        H = H + chunk_metrics[\"hits\"]\n",
    "        S = S + chunk_metrics[\"substitutions\"]\n",
    "        D = D + chunk_metrics[\"deletions\"]\n",
    "        I = I + chunk_metrics[\"insertions\"]\n",
    "        start += chunk_size\n",
    "        end += chunk_size\n",
    "\n",
    "        # sometimes this metric uses a lot of memory, so we'll try to free it here\n",
    "        del chunk_metrics\n",
    "        gc.collect()\n",
    "    \n",
    "    return float(S + D + I) / float(H + S + D)\n",
    "\n",
    "\n",
    "def cer(predictions: list[str], references: list[str], chunk_size: Optional[int] = None) -> float:\n",
    "    \"\"\"\n",
    "    Computes CER score of transcribed segments against references.\n",
    "\n",
    "    Character error rate (CER) is a common metric of the performance of an automatic speech recognition system.\n",
    "    CER is similar to Word Error Rate (WER), but operate on character insted of word. Please refer to docs of WER for further information.\n",
    "    \n",
    "    Character error rate can be computed as:\n",
    "    CER = (S + D + I) / N = (S + D + I) / (S + D + C)\n",
    "    where\n",
    "    S is the number of substitutions,\n",
    "    D is the number of deletions,\n",
    "    I is the number of insertions,\n",
    "    C is the number of correct characters,\n",
    "    N is the number of characters in the reference (N=S+D+C).\n",
    "\n",
    "    CER's output is always a number between 0 and 1. This value indicates the percentage of characters that were incorrectly predicted. \n",
    "    The lower the value, the better the performance of the ASR system with a CER of 0 being a perfect score.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "        predictions: list[str]\n",
    "            List of transcribtions to score.\n",
    "        \n",
    "        references: list[str]\n",
    "            List of references for each speech input.\n",
    "        \n",
    "        chunk_size: Optional[int] = None\n",
    "            Size of the chunk to use for computation. \n",
    "            When this value is specified, the function will chunk the data into batches of the specified size and compute the CER on each batch.\n",
    "            After all batches are computed, the function will compute the average CER over all batches.\n",
    "            (You will probably need to define this if you have memory issues).\n",
    "   \n",
    "    Returns:\n",
    "    ----------\n",
    "        float: the character error rate\n",
    "    \"\"\"\n",
    "\n",
    "    if chunk_size is None: return jiwer.cer(references, predictions)\n",
    "\n",
    "    start = 0\n",
    "    end = chunk_size\n",
    "    H, S, D, I = 0, 0, 0, 0\n",
    "\n",
    "    while start < len(references):\n",
    "        chunk_metrics = jiwer.cer(references[start:end], predictions[start:end], return_dict=True)\n",
    "        H = H + chunk_metrics[\"hits\"]\n",
    "        S = S + chunk_metrics[\"substitutions\"]\n",
    "        D = D + chunk_metrics[\"deletions\"]\n",
    "        I = I + chunk_metrics[\"insertions\"]\n",
    "        start += chunk_size\n",
    "        end += chunk_size\n",
    "\n",
    "        # sometimes this metric uses a lot of memory, so we'll try to free it here\n",
    "        del chunk_metrics\n",
    "        gc.collect()\n",
    "    \n",
    "    return float(S + D + I) / float(H + S + D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from kfe.dependencies import get_speech_decoder, SpeechRecognitionModel, try_loading_cached_or_download, AutoModelForCTC, Wav2Vec2Processor, CONFIG_DIR\n",
    "\n",
    "device=torch.device('cuda')\n",
    "\n",
    "# model_id = f\"jonatasgrosman/wav2vec2-large-xlsr-53-polish\"\n",
    "# model = SpeechRecognitionModel(\n",
    "#     model=try_loading_cached_or_download(\n",
    "#         model_id,\n",
    "#         lambda x: AutoModelForCTC.from_pretrained(x.model_path, cache_dir=x.cache_dir, local_files_only=x.local_files_only)\n",
    "#     ).to(device),\n",
    "#     processor=try_loading_cached_or_download(model_id, lambda x: Wav2Vec2Processor.from_pretrained(x.model_path, cache_dir=x.cache_dir, local_files_only=x.local_files_only)),\n",
    "#     device=device\n",
    "# )\n",
    "\n",
    "model = SpeechRecognitionModel(\n",
    "    # model=AutoModelForCTC.from_pretrained(CONFIG_DIR.joinpath('finetuned_pl_speech_model')).to(device),\n",
    "    model=AutoModelForCTC.from_pretrained('./speech_model_finetuned').to(device),\n",
    "    processor=Wav2Vec2Processor.from_pretrained('./speech_model_finetuned'),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "decoder = get_speech_decoder(model, 'pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./speech_finetuning_data/test_data_split.json', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "predictions, references = [], []\n",
    "for item in test_data:\n",
    "    path, transcription = item['path'], item['transcription']\n",
    "    references.append(transcription)\n",
    "    predictions.append(model.transcribe([path], decoder=decoder)[0]['transcription'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cer:', cer(predictions, references))\n",
    "print('wer:', wer(predictions, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cer:', cer(predictions, references))\n",
    "print('wer:', wer(predictions, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cer:', cer(predictions, references))\n",
    "print('wer:', wer(predictions, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cer:', cer(predictions, references))\n",
    "print('wer:', wer(predictions, references))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
